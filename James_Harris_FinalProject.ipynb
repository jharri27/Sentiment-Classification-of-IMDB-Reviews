{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # James D. Harris - IST 664 Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import pandas\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.corpus import sentence_polarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus.reader.api import *\n",
    "from nltk.tokenize import *\n",
    "from nltk.collocations import *\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory path\n",
    "\n",
    "dirPath = 'C:/Users/Jack Harris/! Syracuse Python/IST 664/kagglemoviereviews/corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: define features\n",
    "\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: cross-validation\n",
    "\n",
    "def cross_validation_PRF(num_folds, featuresets, labels):\n",
    "    subset_size = int(len(featuresets)/num_folds)\n",
    "    print('Each fold size:', subset_size)\n",
    "    # for the number of labels - start the totals lists with zeroes\n",
    "    num_labels = len(labels)\n",
    "    total_precision_list = [0] * num_labels\n",
    "    total_recall_list = [0] * num_labels\n",
    "    total_F1_list = [0] * num_labels\n",
    "\n",
    "    # iterate over the folds\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[(i*subset_size):][:subset_size]\n",
    "        train_this_round = featuresets[:(i*subset_size)] + featuresets[((i+1)*subset_size):]\n",
    "        # train using train_this_round\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        # evaluate against test_this_round to produce the gold and predicted labels\n",
    "        goldlist = []\n",
    "        predictedlist = []\n",
    "        for (features, label) in test_this_round:\n",
    "            goldlist.append(label)\n",
    "            predictedlist.append(classifier.classify(features))\n",
    "\n",
    "        # computes evaluation measures for this fold and\n",
    "        #   returns list of measures for each label\n",
    "        print('Fold', i)\n",
    "        (precision_list, recall_list, F1_list) \\\n",
    "                  = eval_measures(goldlist, predictedlist, labels)\n",
    "        # take off triple string to print precision, recall and F1 for each fold\n",
    "        '''\n",
    "        print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "        # print measures for each label\n",
    "        for i, lab in enumerate(labels):\n",
    "            print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "              \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "        '''\n",
    "        # for each label add to the sums in the total lists\n",
    "        for i in range(num_labels):\n",
    "            # for each label, add the 3 measures to the 3 lists of totals\n",
    "            total_precision_list[i] += precision_list[i]\n",
    "            total_recall_list[i] += recall_list[i]\n",
    "            total_F1_list[i] += F1_list[i]\n",
    "\n",
    "    # find precision, recall and F measure averaged over all rounds for all labels\n",
    "    # compute averages from the totals lists\n",
    "    precision_list = [tot/num_folds for tot in total_precision_list]\n",
    "    recall_list = [tot/num_folds for tot in total_recall_list]\n",
    "    F1_list = [tot/num_folds for tot in total_F1_list]\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\nAverage Precision\\tRecall\\t\\tF1 \\tPer Label')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "    \n",
    "    # print macro average over all labels - treats each label equally\n",
    "    print('\\nMacro Average Precision\\tRecall\\t\\tF1 \\tOver All Labels')\n",
    "    print('\\t', \"{:10.3f}\".format(sum(precision_list)/num_labels), \\\n",
    "          \"{:10.3f}\".format(sum(recall_list)/num_labels), \\\n",
    "          \"{:10.3f}\".format(sum(F1_list)/num_labels))\n",
    "\n",
    "    # for micro averaging, weight the scores for each label by the number of items\n",
    "    #    this is better for labels with imbalance\n",
    "    # first intialize a dictionary for label counts and then count them\n",
    "    label_counts = {}\n",
    "    for lab in labels:\n",
    "      label_counts[lab] = 0 \n",
    "    # count the labels\n",
    "    for (doc, lab) in featuresets:\n",
    "      label_counts[lab] += 1\n",
    "    # make weights compared to the number of documents in featuresets\n",
    "    num_docs = len(featuresets)\n",
    "    label_weights = [(label_counts[lab] / num_docs) for lab in labels]\n",
    "    print('\\nLabel Counts', label_counts)\n",
    "    #print('Label weights', label_weights)\n",
    "    # print macro average over all labels\n",
    "    print('Micro Average Precision\\tRecall\\t\\tF1 \\tOver All Labels')\n",
    "    precision = sum([a * b for a,b in zip(precision_list, label_weights)])\n",
    "    recall = sum([a * b for a,b in zip(recall_list, label_weights)])\n",
    "    F1 = sum([a * b for a,b in zip(F1_list, label_weights)])\n",
    "    print( '\\t', \"{:10.3f}\".format(precision), \\\n",
    "      \"{:10.3f}\".format(recall), \"{:10.3f}\".format(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: compute precision, recall, and F1\n",
    "\n",
    "def eval_measures(gold, predicted, labels):\n",
    "    \n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        # for small numbers, guard against dividing by zero in computing measures\n",
    "        if (TP == 0) or (FP == 0) or (FN == 0):\n",
    "          recall_list.append (0)\n",
    "          precision_list.append (0)\n",
    "          F1_list.append(0)\n",
    "        else:\n",
    "          recall = TP / (TP + FP)\n",
    "          precision = TP / (TP + FN)\n",
    "          recall_list.append(recall)\n",
    "          precision_list.append(precision)\n",
    "          F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    return (precision_list, recall_list, F1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: define bigram features\n",
    "\n",
    "def bigram_document_features(document, word_features, bigram_features):\n",
    "    document_words = set(document)\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    for bigram in bigram_features:\n",
    "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: write featuresets to CSV file\n",
    "\n",
    "def writeFeatureSets(featuresets, outpath):\n",
    "    # Open outpath for writing\n",
    "    f = open(outpath, 'w')\n",
    "    # Get the feature names from the feature dictionary in the first featureset\n",
    "    featurenames = featuresets[0][0].keys()\n",
    "    # Create the first list of the file as comma-separated feature names\n",
    "    #   with the word class as the last feature name\n",
    "    featurenameline = ''\n",
    "    for featurename in featurenames:\n",
    "        # Replace forbidden characters with text abbreviations\n",
    "        featurename = featurename.replace(',','CM')\n",
    "        featurename = featurename.replace(\"'\",\"DQ\")\n",
    "        featurename = featurename.replace('\"','QU')\n",
    "        featurenameline += featurename + ','\n",
    "        featurenameline += 'class'\n",
    "    # Write this as the first line in the csv file\n",
    "    f.write(featurenameline)\n",
    "    f.write('\\n')\n",
    "    # Convert each feature set to a line in the file with comma separated feature values,\n",
    "    # each feature value is converted to a string \n",
    "    #   for booleans this is the words true and false\n",
    "    #   for numbers, this is the string with the number\n",
    "    for featureset in featuresets:\n",
    "        featureline = ''\n",
    "        for key in featurenames:\n",
    "            try:\n",
    "                featureline += str(featureset[0].get(key, []))+','\n",
    "            except KeyError:\n",
    "                continue\n",
    "        featureline += str(featureset[1])\n",
    "        # Write each feature set values to the file\n",
    "        f.write(featureline)\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: define sentiment lexicon features\n",
    "\n",
    "def readSubjectivity(path):\n",
    "\tflexicon = open(path, 'r')\n",
    "\t# initialize an empty dictionary\n",
    "\tsldict = { }\n",
    "\tfor line in flexicon:\n",
    "\t\tfields = line.split()   # default is to split on whitespace\n",
    "\t\t# split each field on the '=' and keep the second part as the value\n",
    "\t\tstrength = fields[0].split(\"=\")[1]\n",
    "\t\tword = fields[2].split(\"=\")[1]\n",
    "\t\tposTag = fields[3].split(\"=\")[1]\n",
    "\t\tstemmed = fields[4].split(\"=\")[1]\n",
    "\t\tpolarity = fields[5].split(\"=\")[1]\n",
    "\t\tif (stemmed == 'y'):\n",
    "\t\t\tisStemmed = True\n",
    "\t\telse:\n",
    "\t\t\tisStemmed = False\n",
    "\t\t# put a dictionary entry with the word as the keyword\n",
    "\t\t#     and a list of the other values\n",
    "\t\tsldict[word] = [strength, posTag, isStemmed, polarity]\n",
    "\treturn sldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: define sentiment lexicon features\n",
    "\n",
    "def SL_features(document, word_features, SL):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    # count variables for the 4 classes of subjectivity\n",
    "    weakPos = 0\n",
    "    strongPos = 0\n",
    "    weakNeg = 0\n",
    "    strongNeg = 0\n",
    "    for word in document_words:\n",
    "        if word in SL:\n",
    "            strength, posTag, isStemmed, polarity = SL[word]\n",
    "            if strength == 'weaksubj' and polarity == 'positive':\n",
    "                weakPos += 1\n",
    "            if strength == 'strongsubj' and polarity == 'positive':\n",
    "                strongPos += 1\n",
    "            if strength == 'weaksubj' and polarity == 'negative':\n",
    "                weakNeg += 1\n",
    "            if strength == 'strongsubj' and polarity == 'negative':\n",
    "                strongNeg += 1\n",
    "            features['positivecount'] = weakPos + (2 * strongPos)\n",
    "            features['negativecount'] = weakNeg + (2 * strongNeg)      \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: retrieve poslist and neglist from LIWC dictionary\n",
    "\n",
    "def read_words():\n",
    "  poslist = []\n",
    "  neglist = []\n",
    "\n",
    "  flexicon = open('M:/Education/Syracuse/IST 664 - Natural Language Processing/Final Project/kagglemoviereviews/SentimentLexicons/liwcdic2007.dic', encoding='latin1')\n",
    "  # read all LIWC words from file\n",
    "  wordlines = [line.strip() for line in flexicon]\n",
    "  # each line has a word or a stem followed by * and numbers of the word classes it is in\n",
    "  # word class 126 is positive emotion and 127 is negative emotion\n",
    "  for line in wordlines:\n",
    "    if not line == '':\n",
    "      items = line.split()\n",
    "      word = items[0]\n",
    "      classes = items[1:]\n",
    "      for c in classes:\n",
    "        if c == '126':\n",
    "          poslist.append( word )\n",
    "        if c == '127':\n",
    "          neglist.append( word )\n",
    "  return (poslist, neglist)\n",
    "\n",
    "poslist, neglist = read_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: define LIWC sentiment lexicon features\n",
    "\n",
    "def liwc_features(doc, word_features,poslist,neglist):\n",
    "  doc_words = set(doc)\n",
    "  features = {}\n",
    "  for word in word_features:\n",
    "    features['contains({})'.format(word)] = (word in doc_words)\n",
    "  pos = 0\n",
    "  neg = 0\n",
    "  for word in doc_words:\n",
    "    if isPresent(word,poslist):\n",
    "      pos += 1\n",
    "    if isPresent(word,neglist):\n",
    "      neg += 1\n",
    "    features['positivecount'] = pos\n",
    "    features['negativecount'] = neg\n",
    "  if 'positivecount' not in features:\n",
    "    features['positivecount']=0\n",
    "  if 'negativecount' not in features:\n",
    "    features['negativecount']=0  \n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: define a combination of SL and LIWC lexicons\n",
    "\n",
    "def SL_liwc_features(doc, word_features, SL,poslist,neglist):\n",
    "  document_words = set(doc)\n",
    "  features = {}\n",
    "  for word in word_features:\n",
    "    features['contains({})'.format(word)] = (word in document_words)\n",
    "  # count variables for the 4 classes of subjectivity\n",
    "  weakPos = 0\n",
    "  strongPos = 0\n",
    "  weakNeg = 0\n",
    "  strongNeg = 0\n",
    "  for word in document_words:\n",
    "    if isPresent(word,poslist):\n",
    "      strongPos += 1\n",
    "    elif isPresent(word,neglist):\n",
    "      strongNeg += 1\n",
    "    elif word in SL:\n",
    "      strength, posTag, isStemmed, polarity = SL[word]\n",
    "      if strength == 'weaksubj' and polarity == 'positive':\n",
    "        weakPos += 1\n",
    "      if strength == 'strongsubj' and polarity == 'positive':\n",
    "        strongPos += 1\n",
    "      if strength == 'weaksubj' and polarity == 'negative':\n",
    "        weakNeg += 1\n",
    "      if strength == 'strongsubj' and polarity == 'negative':\n",
    "        strongNeg += 1\n",
    "    features['positivecount'] = weakPos + (2 * strongPos)\n",
    "    features['negativecount'] = weakNeg + (2 * strongNeg)\n",
    "  \n",
    "  if 'positivecount' not in features:\n",
    "    features['positivecount']=0\n",
    "  if 'negativecount' not in features:\n",
    "    features['negativecount']=0      \n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: define part-of-speech tagging features\n",
    "\n",
    "def POS_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    tagged_words = nltk.pos_tag(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    numNoun = 0\n",
    "    numVerb = 0\n",
    "    numAdj = 0\n",
    "    numAdverb = 0\n",
    "    for (word, tag) in tagged_words:\n",
    "        if tag.startswith('N'): numNoun += 1\n",
    "        if tag.startswith('V'): numVerb += 1\n",
    "        if tag.startswith('J'): numAdj += 1\n",
    "        if tag.startswith('R'): numAdverb += 1\n",
    "    features['nouns'] = numNoun\n",
    "    features['verbs'] = numVerb\n",
    "    features['adjectives'] = numAdj\n",
    "    features['adverbs'] = numAdverb\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Representing negation\n",
    "\n",
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = False\n",
    "        features['V_NOT{}'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['V_NOT{}'.format(document[i])] = (document[i] in word_features)\n",
    "        else:\n",
    "            features['V_{}'.format(word)] = (word in word_features)\n",
    "    return features\n",
    "\n",
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Bing Liu's Opinion Lexicon\n",
    "\n",
    "dirPath = 'C:/Users/Jack Harris/! Syracuse Python/IST 664/kagglemoviereviews'\n",
    "\n",
    "def read_opinionlexicon():\n",
    "    POSITIVE_REVIEWS = 'C:/Users/Jack Harris/! Syracuse Python/IST 664/kagglemoviereviews/SentimentLexicons/rt-polarity-pos.txt'\n",
    "    NEGATIVE_REVIEWS = 'C:/Users/Jack Harris/! Syracuse Python/IST 664/kagglemoviereviews/SentimentLexicons/rt-polarity-pos.txt'\n",
    "    \n",
    "    pos_features = []\n",
    "    neg_features = []\n",
    "    for line in open(POSITIVE_REVIEWS, 'r').readlines()[35:]:\n",
    "        pos_words = re.findall(r\"[\\w']+|[.,!?;]\", line.rstrip())\n",
    "        pos_features.append(pos_words[0])\n",
    "        \n",
    "    for line in open(NEGATIVE_REVIEWS, 'r').readlines()[35:]:\n",
    "        neg_words = re.findall(r\"[\\w']+|[.,!?;]\", line.rstrip())\n",
    "        neg_features.append(neg_words[0])\n",
    "  \n",
    "    return pos_features,neg_features\n",
    "\n",
    "poslist2,neglist2 = read_opinionlexicon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10227\n"
     ]
    }
   ],
   "source": [
    "# I could not get this code to work when encapsulated in the processkaggle() function\n",
    "# Featureset1: Bag of words / unigram (baseline)\n",
    "\n",
    "vocab_size = 500\n",
    "limit = int(10000)\n",
    "os.chdir(dirPath)\n",
    "f = open('./train.tsv', 'r')\n",
    "phrasedata=[]\n",
    "for line in f:\n",
    "    if (not line.startswith('Phrase')):\n",
    "        line = line.strip()\n",
    "        phrasedata.append(line.split('\\t')[2:4])\n",
    "random.shuffle(phrasedata)\n",
    "phraselist = phrasedata[:limit]\n",
    "phrasedocs = []\n",
    "for phrase in phraselist:\n",
    "    tokens = nltk.word_tokenize(phrase[0])\n",
    "    phrasedocs.append((tokens, int(phrase[1])))\n",
    "docs = []\n",
    "for phrase in phrasedocs:\n",
    "    lowerphrase = ([w.lower() for w in phrase[0]], phrase[1])\n",
    "    docs.append(lowerphrase)\n",
    "all_words_list = [word for (sent, cat) in docs for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "print(len(all_words))\n",
    "word_items = all_words.most_common(vocab_size)\n",
    "word_features = [word for (word, count) in word_items]\n",
    "featuresets = [(document_features(d, word_features), c) for (d,c) in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [c for (d,c) in docs]\n",
    "labels = list(set(label_list))\n",
    "num_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.219      0.174      0.194\n",
      "1 \t      0.210      0.358      0.265\n",
      "2 \t      0.815      0.616      0.702\n",
      "3 \t      0.240      0.407      0.302\n",
      "4 \t      0.175      0.254      0.207\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.332      0.362      0.334\n",
      "\n",
      "Label Counts {0: 427, 1: 1744, 2: 5121, 3: 2158, 4: 550}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.525      0.487      0.490\n",
      "125.82843351364136  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featuresets, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.524"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featuresets(baseline)\n",
    "\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.219      0.174      0.194\n",
      "1 \t      0.210      0.358      0.265\n",
      "2 \t      0.815      0.616      0.702\n",
      "3 \t      0.240      0.407      0.302\n",
      "4 \t      0.175      0.254      0.207\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.332      0.362      0.334\n",
      "\n",
      "Label Counts {0: 427, 1: 1744, 2: 5121, 3: 2158, 4: 550}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.525      0.487      0.490\n",
      "256.7317645549774  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset2: bigram\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(all_words_list)\n",
    "bigram_features = finder.nbest(bigram_measures.chi_sq, 500)\n",
    "# print(bigram_features[:50])\n",
    "featuresets2 = [(bigram_document_features(d, word_features, bigram_features), c) for (d, c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featuresets2, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.524"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featuresets2(bigrams)\n",
    "\n",
    "train_set, test_set = featuresets2[1000:], featuresets2[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.258      0.180      0.212\n",
      "1 \t      0.272      0.369      0.313\n",
      "2 \t      0.744      0.661      0.700\n",
      "3 \t      0.354      0.427      0.387\n",
      "4 \t      0.234      0.240      0.236\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.373      0.375      0.370\n",
      "\n",
      "Label Counts {0: 427, 1: 1744, 2: 5121, 3: 2158, 4: 550}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.529      0.516      0.519\n",
      "121.68763709068298  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset3: Sentiment Lexicon\n",
    "\n",
    "SLpath = \"M:/Education/Syracuse/IST 664 - Natural Language Processing/Final Project/kagglemoviereviews/SentimentLexicons/subjclueslen1-HLTEMNLP05.tff\"\n",
    "SL = readSubjectivity(SLpath)\n",
    "\n",
    "featureset3 = [(SL_features(d, word_features, SL), c) for (d, c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset3, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.532"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset3 (sentiment lexicon)\n",
    "\n",
    "train_set, test_set = featureset3[1000:], featureset3[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.230      0.182      0.203\n",
      "1 \t      0.220      0.374      0.276\n",
      "2 \t      0.813      0.631      0.711\n",
      "3 \t      0.270      0.429      0.331\n",
      "4 \t      0.214      0.251      0.230\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.349      0.374      0.350\n",
      "\n",
      "Label Counts {0: 427, 1: 1744, 2: 5121, 3: 2158, 4: 550}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.535      0.503      0.505\n",
      "121.92599964141846  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset4 : LIWC\n",
    "\n",
    "featureset4 = [(liwc_features(d, word_features,poslist,neglist), c) for (d,c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset4, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.534"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset4 (LIWC sentiment lexicon)\n",
    "\n",
    "train_set, test_set = featureset4[1000:], featureset4[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.239      0.184      0.207\n",
      "1 \t      0.235      0.384      0.291\n",
      "2 \t      0.804      0.639      0.712\n",
      "3 \t      0.291      0.438      0.349\n",
      "4 \t      0.218      0.247      0.230\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.357      0.378      0.358\n",
      "\n",
      "Label Counts {0: 427, 1: 1744, 2: 5121, 3: 2158, 4: 550}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.538      0.510      0.512\n",
      "121.13510990142822  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset5: Combination SL and LIWC\n",
    "\n",
    "featureset5 = [(SL_liwc_features(d, word_features, SL, poslist, neglist), c) for (d,c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset5, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.535"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset5 (combination SL and LIWC)\n",
    "\n",
    "train_set, test_set = featureset5[1000:], featureset5[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.287      0.160      0.205\n",
      "1 \t      0.213      0.351      0.265\n",
      "2 \t      0.794      0.624      0.699\n",
      "3 \t      0.218      0.398      0.282\n",
      "4 \t      0.190      0.223      0.204\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.340      0.351      0.331\n",
      "\n",
      "Label Counts {0: 427, 1: 1744, 2: 5121, 3: 2158, 4: 550}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.513      0.486      0.485\n",
      "124.73035550117493  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset 6: Part-of-speech tagging\n",
    "\n",
    "featureset6 = [(POS_features(d, word_features), c) for (d, c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset6, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.514"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset6 (part-of-speech tagging)\n",
    "\n",
    "train_set, test_set = featureset6[1000:], featureset6[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.492      0.139      0.217\n",
      "1 \t      0.216      0.364      0.271\n",
      "2 \t      0.673      0.685      0.679\n",
      "3 \t      0.274      0.422      0.332\n",
      "4 \t      0.344      0.186      0.241\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.400      0.359      0.348\n",
      "\n",
      "Label Counts {0: 427, 1: 1744, 2: 5121, 3: 2158, 4: 550}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.481      0.522      0.489\n",
      "262.31035470962524  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset 7: Representing negation\n",
    "\n",
    "featureset7 = [(NOT_features(d, word_features, negationwords), c) for (d, c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset7, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.505"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset7 (representing negation)\n",
    "\n",
    "train_set, test_set = featureset7[1000:], featureset7[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.264      0.160      0.199\n",
      "1 \t      0.220      0.349      0.269\n",
      "2 \t      0.787      0.622      0.695\n",
      "3 \t      0.223      0.394      0.285\n",
      "4 \t      0.204      0.232      0.216\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.340      0.351      0.333\n",
      "\n",
      "Label Counts {0: 427, 1: 1744, 2: 5121, 3: 2158, 4: 550}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.512      0.484      0.485\n",
      "120.99084901809692  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset 8: Using Bing Liu's Opinion Lexicon, obtained at:\n",
    "#   https://www.cs.uic.edu/~liub/\n",
    "\n",
    "featureset8 = [(liwc_features(d, word_features,poslist2,neglist2), c) for (d,c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset8, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.513"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset8 (Bing Liu's Opinion Lexicon)\n",
    "\n",
    "train_set, test_set = featureset8[1000:], featureset8[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10445\n"
     ]
    }
   ],
   "source": [
    "# Featureset1: Bag of words / unigram (baseline)\n",
    "\n",
    "vocab_size = 1000\n",
    "limit = int(10000)\n",
    "os.chdir(dirPath)\n",
    "f = open('./train.tsv', 'r')\n",
    "phrasedata=[]\n",
    "for line in f:\n",
    "    if (not line.startswith('Phrase')):\n",
    "        line = line.strip()\n",
    "        phrasedata.append(line.split('\\t')[2:4])\n",
    "random.shuffle(phrasedata)\n",
    "phraselist = phrasedata[:limit]\n",
    "phrasedocs = []\n",
    "for phrase in phraselist:\n",
    "    tokens = nltk.word_tokenize(phrase[0])\n",
    "    phrasedocs.append((tokens, int(phrase[1])))\n",
    "docs = []\n",
    "for phrase in phrasedocs:\n",
    "    lowerphrase = ([w.lower() for w in phrase[0]], phrase[1])\n",
    "    docs.append(lowerphrase)\n",
    "all_words_list = [word for (sent, cat) in docs for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "print(len(all_words))\n",
    "word_items = all_words.most_common(vocab_size)\n",
    "word_features = [word for (word, count) in word_items]\n",
    "featuresets = [(document_features(d, word_features), c) for (d,c) in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.285      0.186      0.224\n",
      "1 \t      0.231      0.367      0.283\n",
      "2 \t      0.811      0.629      0.708\n",
      "3 \t      0.253      0.425      0.317\n",
      "4 \t      0.154      0.247      0.189\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.347      0.371      0.344\n",
      "\n",
      "Label Counts {0: 439, 1: 1711, 2: 5150, 3: 2103, 4: 597}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.532      0.499      0.501\n",
      "254.00768756866455  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset 1: Bag of words / unigram (baseline), vocabulary size 1000\n",
    "\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featuresets, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.532"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featuresets(baseline), vocab 1000\n",
    "\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.285      0.186      0.224\n",
      "1 \t      0.231      0.367      0.283\n",
      "2 \t      0.811      0.629      0.708\n",
      "3 \t      0.253      0.425      0.317\n",
      "4 \t      0.154      0.247      0.189\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.347      0.371      0.344\n",
      "\n",
      "Label Counts {0: 439, 1: 1711, 2: 5150, 3: 2103, 4: 597}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.532      0.499      0.501\n",
      "406.28346014022827  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset2: bigram, vocab 1000\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(all_words_list)\n",
    "bigram_features = finder.nbest(bigram_measures.chi_sq, 500)\n",
    "# print(bigram_features[:50])\n",
    "featuresets2 = [(bigram_document_features(d, word_features, bigram_features), c) for (d, c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featuresets2, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.532"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featuresets2(bigrams), vocab 1000\n",
    "\n",
    "train_set, test_set = featuresets2[1000:], featuresets2[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.306      0.195      0.237\n",
      "1 \t      0.266      0.362      0.307\n",
      "2 \t      0.754      0.671      0.711\n",
      "3 \t      0.352      0.429      0.387\n",
      "4 \t      0.242      0.266      0.253\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.384      0.385      0.379\n",
      "\n",
      "Label Counts {0: 439, 1: 1711, 2: 5150, 3: 2103, 4: 597}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.536      0.522      0.525\n",
      "271.8794481754303  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset3: Sentiment Lexicon, vocab 1000\n",
    "\n",
    "SLpath = \"M:/Education/Syracuse/IST 664 - Natural Language Processing/Final Project/kagglemoviereviews/SentimentLexicons/subjclueslen1-HLTEMNLP05.tff\"\n",
    "SL = readSubjectivity(SLpath)\n",
    "\n",
    "featureset3 = [(SL_features(d, word_features, SL), c) for (d, c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset3, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.537"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset3 (sentiment lexicon), vocab 1000\n",
    "\n",
    "train_set, test_set = featureset3[1000:], featureset3[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.292      0.196      0.233\n",
      "1 \t      0.223      0.368      0.277\n",
      "2 \t      0.806      0.642      0.714\n",
      "3 \t      0.292      0.439      0.351\n",
      "4 \t      0.198      0.265      0.226\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.362      0.382      0.360\n",
      "\n",
      "Label Counts {0: 439, 1: 1711, 2: 5150, 3: 2103, 4: 597}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.539      0.510      0.513\n",
      "282.2764194011688  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset4 : LIWC, vocab 1000\n",
    "\n",
    "featureset4 = [(liwc_features(d, word_features,poslist,neglist), c) for (d,c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset4, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.535"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset4 (LIWC sentiment lexicon), vocab 1000\n",
    "\n",
    "train_set, test_set = featureset4[1000:], featureset4[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.294      0.197      0.235\n",
      "1 \t      0.238      0.390      0.295\n",
      "2 \t      0.806      0.650      0.720\n",
      "3 \t      0.307      0.443      0.363\n",
      "4 \t      0.217      0.276      0.242\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.372      0.391      0.371\n",
      "\n",
      "Label Counts {0: 439, 1: 1711, 2: 5150, 3: 2103, 4: 597}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.546      0.520      0.522\n",
      "273.61738204956055  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset5: Combination SL and LIWC, vocab 1000\n",
    "\n",
    "featureset5 = [(SL_liwc_features(d, word_features, SL, poslist, neglist), c) for (d,c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset5, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.541"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset5 (combination SL and LIWC), vocab 1000\n",
    "\n",
    "train_set, test_set = featureset5[1000:], featureset5[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.328      0.170      0.223\n",
      "1 \t      0.220      0.349      0.270\n",
      "2 \t      0.799      0.636      0.708\n",
      "3 \t      0.228      0.416      0.295\n",
      "4 \t      0.179      0.234      0.202\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.351      0.361      0.340\n",
      "\n",
      "Label Counts {0: 439, 1: 1711, 2: 5150, 3: 2103, 4: 597}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.522      0.496      0.495\n",
      "275.3955705165863  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset 6: Part-of-speech tagging, vocab 1000\n",
    "\n",
    "featureset6 = [(POS_features(d, word_features), c) for (d, c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset6, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.518"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset6 (part-of-speech tagging), vocab 1000\n",
    "\n",
    "train_set, test_set = featureset6[1000:], featureset6[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.459      0.160      0.236\n",
      "1 \t      0.256      0.391      0.309\n",
      "2 \t      0.725      0.680      0.702\n",
      "3 \t      0.260      0.419      0.320\n",
      "4 \t      0.309      0.224      0.259\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.402      0.375      0.365\n",
      "\n",
      "Label Counts {0: 439, 1: 1711, 2: 5150, 3: 2103, 4: 597}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.510      0.526      0.508\n",
      "563.2704789638519  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset 7: Representing negation, vocab 1000\n",
    "\n",
    "featureset7 = [(NOT_features(d, word_features, negationwords), c) for (d, c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset7, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.492"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset7 (representing negation), vocab 1000\n",
    "\n",
    "train_set, test_set = featureset7[1000:], featureset7[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.322      0.171      0.222\n",
      "1 \t      0.211      0.346      0.262\n",
      "2 \t      0.789      0.633      0.702\n",
      "3 \t      0.240      0.405      0.301\n",
      "4 \t      0.187      0.240      0.209\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.350      0.359      0.339\n",
      "\n",
      "Label Counts {0: 439, 1: 1711, 2: 5150, 3: 2103, 4: 597}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.518      0.492      0.492\n",
      "262.74332761764526  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset 8: Using Bing Liu's Opinion Lexicon, vocab 1000\n",
    "\n",
    "featureset8 = [(liwc_features(d, word_features,poslist2,neglist2), c) for (d,c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset8, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.512"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset8 (Bing Liu's Opinion Lexicon), vocab 1000\n",
    "\n",
    "train_set, test_set = featureset8[1000:], featureset8[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10338\n",
      "Each fold size: 1000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.282      0.196      0.230\n",
      "1 \t      0.182      0.349      0.239\n",
      "2 \t      0.822      0.623      0.709\n",
      "3 \t      0.254      0.387      0.307\n",
      "4 \t      0.197      0.356      0.252\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.348      0.382      0.347\n",
      "\n",
      "Label Counts {0: 442, 1: 1710, 2: 5087, 3: 2130, 4: 631}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.528      0.490      0.493\n",
      "191.06860423088074  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset1: Bag of words / unigram (baseline), vocab 500, 10 folds\n",
    "\n",
    "vocab_size = 500\n",
    "limit = int(10000)\n",
    "os.chdir(dirPath)\n",
    "f = open('./train.tsv', 'r')\n",
    "phrasedata=[]\n",
    "for line in f:\n",
    "    if (not line.startswith('Phrase')):\n",
    "        line = line.strip()\n",
    "        phrasedata.append(line.split('\\t')[2:4])\n",
    "random.shuffle(phrasedata)\n",
    "phraselist = phrasedata[:limit]\n",
    "phrasedocs = []\n",
    "for phrase in phraselist:\n",
    "    tokens = nltk.word_tokenize(phrase[0])\n",
    "    phrasedocs.append((tokens, int(phrase[1])))\n",
    "docs = []\n",
    "for phrase in phrasedocs:\n",
    "    lowerphrase = ([w.lower() for w in phrase[0]], phrase[1])\n",
    "    docs.append(lowerphrase)\n",
    "all_words_list = [word for (sent, cat) in docs for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "print(len(all_words))\n",
    "word_items = all_words.most_common(vocab_size)\n",
    "word_features = [word for (word, count) in word_items]\n",
    "featuresets = [(document_features(d, word_features), c) for (d,c) in docs]\n",
    "\n",
    "num_folds = 10\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featuresets, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.529"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featuresets(baseline), vocab 500, 10 folds\n",
    "\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10340\n",
      "Each fold size: 1000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.239      0.180      0.204\n",
      "1 \t      0.251      0.370      0.298\n",
      "2 \t      0.822      0.634      0.716\n",
      "3 \t      0.267      0.452      0.336\n",
      "4 \t      0.182      0.288      0.220\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.352      0.385      0.355\n",
      "\n",
      "Label Counts {0: 434, 1: 1745, 2: 5076, 3: 2160, 4: 585}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.540      0.509      0.510\n",
      "409.29962730407715  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset1: Bag of words / unigram (baseline), vocab 1000, 10 folds\n",
    "\n",
    "vocab_size = 1000\n",
    "limit = int(10000)\n",
    "os.chdir(dirPath)\n",
    "f = open('./train.tsv', 'r')\n",
    "phrasedata=[]\n",
    "for line in f:\n",
    "    if (not line.startswith('Phrase')):\n",
    "        line = line.strip()\n",
    "        phrasedata.append(line.split('\\t')[2:4])\n",
    "random.shuffle(phrasedata)\n",
    "phraselist = phrasedata[:limit]\n",
    "phrasedocs = []\n",
    "for phrase in phraselist:\n",
    "    tokens = nltk.word_tokenize(phrase[0])\n",
    "    phrasedocs.append((tokens, int(phrase[1])))\n",
    "docs = []\n",
    "for phrase in phrasedocs:\n",
    "    lowerphrase = ([w.lower() for w in phrase[0]], phrase[1])\n",
    "    docs.append(lowerphrase)\n",
    "all_words_list = [word for (sent, cat) in docs for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "print(len(all_words))\n",
    "word_items = all_words.most_common(vocab_size)\n",
    "word_features = [word for (word, count) in word_items]\n",
    "featuresets = [(document_features(d, word_features), c) for (d,c) in docs]\n",
    "\n",
    "num_folds = 10\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featuresets, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.538"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featuresets(baseline), vocab 1000, 10 folds\n",
    "\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10417\n",
      "Each fold size: 1000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.194      0.157      0.172\n",
      "1 \t      0.260      0.396      0.313\n",
      "2 \t      0.816      0.619      0.704\n",
      "3 \t      0.254      0.435      0.320\n",
      "4 \t      0.217      0.300      0.250\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.348      0.381      0.352\n",
      "\n",
      "Label Counts {0: 425, 1: 1858, 2: 4980, 3: 2144, 4: 593}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.530      0.500      0.499\n",
      "594.736902475357  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset1: Bag of words / unigram (baseline), vocab 1500, 10 folds\n",
    "\n",
    "vocab_size = 1500\n",
    "limit = int(10000)\n",
    "os.chdir(dirPath)\n",
    "f = open('./train.tsv', 'r')\n",
    "phrasedata=[]\n",
    "for line in f:\n",
    "    if (not line.startswith('Phrase')):\n",
    "        line = line.strip()\n",
    "        phrasedata.append(line.split('\\t')[2:4])\n",
    "random.shuffle(phrasedata)\n",
    "phraselist = phrasedata[:limit]\n",
    "phrasedocs = []\n",
    "for phrase in phraselist:\n",
    "    tokens = nltk.word_tokenize(phrase[0])\n",
    "    phrasedocs.append((tokens, int(phrase[1])))\n",
    "docs = []\n",
    "for phrase in phrasedocs:\n",
    "    lowerphrase = ([w.lower() for w in phrase[0]], phrase[1])\n",
    "    docs.append(lowerphrase)\n",
    "all_words_list = [word for (sent, cat) in docs for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "print(len(all_words))\n",
    "word_items = all_words.most_common(vocab_size)\n",
    "word_features = [word for (word, count) in word_items]\n",
    "featuresets = [(document_features(d, word_features), c) for (d,c) in docs]\n",
    "\n",
    "num_folds = 10\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featuresets, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.552"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featuresets(baseline), vocab 1500, 10 folds\n",
    "\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
